{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿QUE ES UNA API?**\n",
    "\n",
    "Un intermediario o interfaz entre sistemas y aplicaciones.\n",
    "reglas y protocolos que permiten que diferentes aplicaciones y sistemas se comuniquen entre sí. Es una interfaz que especifica cómo diferentes componentes de software deben interactuar y compartir datos, funciones y servicios. \n",
    "\n",
    "Las API se utilizan ampliamente en el desarrollo de software para facilitar la integración entre diferentes sistemas.\n",
    "\n",
    "En resumen, las API son puentes de comunicación que permiten que diferentes aplicaciones y servicios se conecten y trabajen juntos de manera eficiente. \n",
    "\n",
    "SOLICITAR DATOS, ENVIAR DATOS O INSTRUCCIONES Y/O ACCEDER A FUNCIONALIDADES.\n",
    "\n",
    " - los recursos se llaman ENPOINTS\n",
    "\n",
    "    *  **GET -- trae datos **\n",
    "    *  ** POST-- envia y crea datos **\n",
    "    * PUT -- Actualizar \n",
    "    * DELETE --- borrar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " status.code \n",
    " - 200 = correcto\n",
    " - 400 = mi codigo esta mal\n",
    " - 500 = error del servidor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cada Api es distinta y hay que leer la documentacion para saber bien como hacer las peticiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dato curioso**\n",
    "\n",
    "Utilizar nombres de tablas en minúsculas es una práctica estándar en Redshift y en muchos sistemas de bases de datos para evitar problemas relacionados con la sensibilidad a las mayúsculas y minúsculas. Esto ayuda a prevenir conflictos y asegura que los nombres de las tablas sean interpretados de manera uniforme, independientemente de cómo se definieron originalmente. \n",
    "\n",
    "Mantener los nombres de tablas con mayúsculas por razones de legibilidad o consistencia con otros sistemas, podrías explorar la opción de realizar conversiones de mayúsculas a minúsculas en tus consultas SQL, utilizando funciones como  LOWER(). Sin embargo, esta solución puede ser menos eficiente y más propensa a errores en comparación con la solución de utilizar nombres de tablas en minúsculas desde el principio. \n",
    "\n",
    "En resumen, la solución de utilizar nombres de tablas en minúsculas en todo tu código y consultas SQL es la manera más sólida y recomendada de abordar este problema y asegurar que tus operaciones en la base de datos Redshift funcionen sin problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***tabla staging*** \n",
    "\n",
    "Una tabla de staging (o tabla de preparación) es una estructura de base de datos utilizada para almacenar temporalmente datos antes de que sean procesados, transformados y cargados en su ubicación final en un sistema de almacenamiento o base de datos. Este proceso es común en la integración de datos, especialmente en el contexto de la extracción, transformación y carga \n",
    "\n",
    "Aquí hay un desglose de los pasos involucrados: \n",
    "\n",
    "    Extracción (Extract) : Los datos se extraen de diversas fuentes, como bases de datos, archivos planos, hojas de cálculo, API web, etc. \n",
    "\n",
    "    Transformación (Transform) : Los datos extraídos se someten a procesos de limpieza, validación, enriquecimiento y transformación para garantizar que cumplan con los requisitos y formatos deseados. Esto puede incluir cambios de formato, cálculos, combinación de datos de diferentes fuentes y otras operaciones. \n",
    "\n",
    "    Carga (Load) : Después de la transformación, los datos se cargan en la tabla de staging en la base de datos. Esta tabla de staging generalmente tiene una estructura que permite acomodar datos crudos o semi-procesados, y suele ser menos optimizada en términos de rendimiento que las tablas finales. \n",
    "\n",
    "    Carga Final (Final Load) : Una vez que los datos se han transformado y validado adecuadamente en la tabla de staging, se pueden cargar en las tablas finales que se utilizan para análisis, informes u otras aplicaciones. Este proceso implica mover los datos de la tabla de staging a su ubicación final. \n",
    "\n",
    "La tabla de staging es importante ya que permite a los analistas y desarrolladores verificar, corregir y validar los datos antes de cargarlos en las tablas finales, evitando la contaminación de datos incorrectos o incompletos en la base de datos principal. También facilita la detección de errores en las etapas tempranas del proceso ETL. \n",
    "\n",
    "En resumen, una tabla de staging es una estructura temporal que juega un papel fundamental en el proceso de integración y gestión de datos, asegurando la calidad y la integridad de los datos antes de que sean utilizados para análisis y toma de decisiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para integrar un contenedor Docker en un flujo de trabajo DAG (Directed Acyclic Graph) en Apache Airflow, puedes seguir estos pasos generales:\n",
    "\n",
    "Configurar Apache Airflow:\n",
    "Asegúrate de tener Apache Airflow instalado y configurado en tu entorno. Debes tener una instancia de Apache Airflow en ejecución.\n",
    "\n",
    "Crea un Directorio para tu DAG:\n",
    "En tu proyecto de Airflow, crea un directorio donde almacenarás tus archivos DAG. Los archivos DAG son scripts de Python que definen las tareas y el flujo de trabajo que deseas ejecutar.\n",
    "\n",
    "Escribe el DAG:\n",
    "En el directorio creado en el paso anterior, escribe un archivo Python que defina tu DAG. En este archivo, importa los módulos de Airflow necesarios y define las tareas que compondrán tu flujo de trabajo.\n",
    "\n",
    "A continuación, te proporciono un ejemplo básico de un DAG que ejecuta un contenedor Docker:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from airflow import DAG\n",
    "from airflow.operators.docker_operator import DockerOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define el nombre de tu DAG y su descripción\n",
    "dag = DAG(\n",
    "    'mi_dag_con_contenedor_docker',\n",
    "    description='Ejemplo de DAG con DockerOperator',\n",
    "    schedule_interval=None,  # Puedes especificar la programación aquí\n",
    "    start_date=datetime(2023, 1, 1),  # Fecha de inicio\n",
    "    catchup=False,  # Para evitar la ejecución de tareas pasadas\n",
    ")\n",
    "\n",
    "# Define una tarea que ejecutará un contenedor Docker\n",
    "tarea_contenedor = DockerOperator(\n",
    "    task_id='ejecutar_contenedor',\n",
    "    image='nombre_de_la_imagen_docker:tag',  # Reemplaza con el nombre de tu imagen Docker\n",
    "    api_version='auto',\n",
    "    auto_remove=True,  # Puedes ajustar esto según tus necesidades\n",
    "    command='/comando_en_el_contenedor',  # Comando que se ejecutará en el contenedor\n",
    "    network_mode='bridge',  # Puedes ajustar la configuración de red si es necesario\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define la secuencia de tareas\n",
    "tarea_contenedor\n",
    "Asegúrate de reemplazar \"nombre_de_la_imagen_docker:tag\" con el nombre de la imagen Docker que deseas ejecutar y \"/comando_en_el_contenedor\" con el comando que deseas ejecutar en el contenedor.\n",
    "\n",
    "Coloca el Archivo DAG en el Directorio de DAGs de Airflow:\n",
    "Copia el archivo Python que contiene tu DAG en el directorio de DAGs de Airflow. Por defecto, este directorio suele estar en alguna ubicación como /opt/airflow/dags o /usr/local/airflow/dags, dependiendo de tu instalación de Airflow.\n",
    "\n",
    "Inicia la Ejecución del DAG:\n",
    "Inicia la ejecución de tu DAG a través de la interfaz de usuario de Airflow o utilizando la línea de comandos de Airflow. Una vez que inicies la ejecución, Airflow programará y ejecutará las tareas según lo definido en tu DAG.\n",
    "\n",
    "Monitorear y Gestionar:\n",
    "Utiliza el panel de control de Airflow para monitorear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.docker_operator import DockerOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define el nombre de tu DAG y su descripción\n",
    "dag = DAG(\n",
    "    'mi_dag_con_contenedor_docker',\n",
    "    description='Ejemplo de DAG con DockerOperator',\n",
    "    schedule_interval=None,  # Puedes especificar la programación aquí\n",
    "    start_date=datetime(2023, 1, 1),  # Fecha de inicio\n",
    "    catchup=False,  # Para evitar la ejecución de tareas pasadas\n",
    ")\n",
    "\n",
    "# Define una tarea que ejecutará un contenedor Docker\n",
    "tarea_contenedor = DockerOperator(\n",
    "    task_id='ejecutar_contenedor',\n",
    "    image='nombre_de_la_imagen_docker:tag',  # Reemplaza con el nombre de tu imagen Docker\n",
    "    api_version='auto',\n",
    "    auto_remove=True,  # Puedes ajustar esto según tus necesidades\n",
    "    command='/comando_en_el_contenedor',  # Comando que se ejecutará en el contenedor\n",
    "    network_mode='bridge',  # Puedes ajustar la configuración de red si es necesario\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define la secuencia de tareas\n",
    "tarea_contenedor\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
